{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 23812,
          "sourceType": "datasetVersion",
          "datasetId": 17810
        }
      ],
      "dockerImageVersionId": 30587,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "T_-r1C1yrhFL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Câu 1"
      ],
      "metadata": {
        "id": "iWmC1veNrhFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VGG16"
      ],
      "metadata": {
        "id": "IujV8LOZrhFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/kaggle/input/chest-xray-pneumonia\""
      ],
      "metadata": {
        "trusted": true,
        "id": "SWZ5-acDrhFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "Chest_XRay_IMG_SIZE = 227\n",
        "channel = 3\n",
        "Chest_XRay_BATCH_SIZE = 256\n",
        "Chest_XRay_COLOR_MODE = 'rgb'"
      ],
      "metadata": {
        "trusted": true,
        "id": "VJ8qUd49rhFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "trusted": true,
        "id": "Oemh4YA2rhFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chest_Xray_Path = '/kaggle/input/chest-xray-pneumonia/chest_xray'"
      ],
      "metadata": {
        "trusted": true,
        "id": "zVJj4x-BrhFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Chest_XRay_CLASSNAMES = {'NORMAL', 'PNEUMONIA'}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-02T08:08:58.405140Z",
          "iopub.execute_input": "2023-12-02T08:08:58.405537Z",
          "iopub.status.idle": "2023-12-02T08:08:58.415659Z",
          "shell.execute_reply.started": "2023-12-02T08:08:58.405493Z",
          "shell.execute_reply": "2023-12-02T08:08:58.414274Z"
        },
        "trusted": true,
        "id": "XTnp___jrhFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chest_Xray_train_set = image_dataset_from_directory(\n",
        "    chest_Xray_Path + '/train',\n",
        "    labels = 'inferred',\n",
        "    label_mode = 'binary',\n",
        "    class_names = Chest_XRay_CLASSNAMES,\n",
        "    color_mode = Chest_XRay_COLOR_MODE,\n",
        "    batch_size = Chest_XRay_BATCH_SIZE,\n",
        "    image_size = (Chest_XRay_IMG_SIZE, Chest_XRay_IMG_SIZE),\n",
        "    interpolation = 'bilinear'\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-02T08:08:58.417288Z",
          "iopub.execute_input": "2023-12-02T08:08:58.417683Z",
          "iopub.status.idle": "2023-12-02T08:08:58.865189Z",
          "shell.execute_reply.started": "2023-12-02T08:08:58.417651Z",
          "shell.execute_reply": "2023-12-02T08:08:58.864036Z"
        },
        "trusted": true,
        "id": "UKIdqRYMrhFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chest_Xray_test_set = image_dataset_from_directory(\n",
        "    chest_Xray_Path + '/test',\n",
        "    labels = 'inferred',\n",
        "    label_mode = 'binary',\n",
        "    class_names = Chest_XRay_CLASSNAMES,\n",
        "    color_mode = Chest_XRay_COLOR_MODE,\n",
        "    batch_size = Chest_XRay_BATCH_SIZE,\n",
        "    image_size = (Chest_XRay_IMG_SIZE, Chest_XRay_IMG_SIZE),\n",
        "    interpolation = 'bilinear'\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-02T08:08:58.866798Z",
          "iopub.execute_input": "2023-12-02T08:08:58.867291Z",
          "iopub.status.idle": "2023-12-02T08:08:59.159905Z",
          "shell.execute_reply.started": "2023-12-02T08:08:58.867246Z",
          "shell.execute_reply": "2023-12-02T08:08:59.158663Z"
        },
        "trusted": true,
        "id": "Jt8y1v5NrhFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chest_Xray_dev_set = image_dataset_from_directory(\n",
        "    chest_Xray_Path + '/val',\n",
        "    labels = 'inferred',\n",
        "    label_mode = 'binary',\n",
        "    class_names = Chest_XRay_CLASSNAMES,\n",
        "    color_mode = Chest_XRay_COLOR_MODE,\n",
        "    batch_size = Chest_XRay_BATCH_SIZE,\n",
        "    image_size = (Chest_XRay_IMG_SIZE, Chest_XRay_IMG_SIZE),\n",
        "    interpolation = 'bilinear'\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-02T08:08:59.163747Z",
          "iopub.execute_input": "2023-12-02T08:08:59.164127Z",
          "iopub.status.idle": "2023-12-02T08:08:59.212426Z",
          "shell.execute_reply.started": "2023-12-02T08:08:59.164097Z",
          "shell.execute_reply": "2023-12-02T08:08:59.211080Z"
        },
        "trusted": true,
        "id": "sM4v-qUurhFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.figure(figsize = (25,10))\n",
        "for images, labels in chest_Xray_train_set.take(1):\n",
        "    for i in range(10):\n",
        "        plt.subplot(2 ,5, i+ 1)\n",
        "        plt.imshow(np.squeeze(images[i].numpy().astype('uint8')))\n",
        "        plt.axis('off')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-02T08:08:59.214268Z",
          "iopub.execute_input": "2023-12-02T08:08:59.215879Z",
          "iopub.status.idle": "2023-12-02T08:09:15.155677Z",
          "shell.execute_reply.started": "2023-12-02T08:08:59.215830Z",
          "shell.execute_reply": "2023-12-02T08:09:15.154128Z"
        },
        "trusted": true,
        "id": "h977nhKQrhFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "vgg = VGG16(include_top = False, input_shape=(227,227,3))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-01T12:04:23.619709Z",
          "iopub.execute_input": "2023-12-01T12:04:23.620122Z",
          "iopub.status.idle": "2023-12-01T12:04:24.584713Z",
          "shell.execute_reply.started": "2023-12-01T12:04:23.620085Z",
          "shell.execute_reply": "2023-12-01T12:04:24.583813Z"
        },
        "trusted": true,
        "id": "ffxyHUlorhFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dong bang cac lop truoc do\n",
        "for layer in vgg.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-01T12:08:32.430142Z",
          "iopub.execute_input": "2023-12-01T12:08:32.430571Z",
          "iopub.status.idle": "2023-12-01T12:08:32.437424Z",
          "shell.execute_reply.started": "2023-12-01T12:08:32.430539Z",
          "shell.execute_reply": "2023-12-01T12:08:32.436063Z"
        },
        "trusted": true,
        "id": "QJpd78jSrhFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tinh chinh dau ra\n",
        "from keras.layers import Flatten, Dense, Input\n",
        "from keras.models import Model\n",
        "\n",
        "flat = Flatten()(vgg.layers[-1].output)\n",
        "fc1 = Dense(1024, activation = 'relu')(flat)\n",
        "output = Dense(1, activation = 'sigmoid')(fc1)\n",
        "\n",
        "model = Model(inputs = vgg.inputs, outputs = output)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-01T12:09:04.759311Z",
          "iopub.execute_input": "2023-12-01T12:09:04.759691Z",
          "iopub.status.idle": "2023-12-01T12:09:04.971462Z",
          "shell.execute_reply.started": "2023-12-01T12:09:04.759663Z",
          "shell.execute_reply": "2023-12-01T12:09:04.970324Z"
        },
        "trusted": true,
        "id": "OvrIuH3drhFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-01T12:09:34.613198Z",
          "iopub.execute_input": "2023-12-01T12:09:34.613634Z",
          "iopub.status.idle": "2023-12-01T12:09:34.675831Z",
          "shell.execute_reply.started": "2023-12-01T12:09:34.613600Z",
          "shell.execute_reply": "2023-12-01T12:09:34.674793Z"
        },
        "trusted": true,
        "id": "7LgKWRlrrhFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-01T12:09:46.367553Z",
          "iopub.execute_input": "2023-12-01T12:09:46.367944Z",
          "iopub.status.idle": "2023-12-01T12:09:46.388629Z",
          "shell.execute_reply.started": "2023-12-01T12:09:46.367912Z",
          "shell.execute_reply": "2023-12-01T12:09:46.387696Z"
        },
        "trusted": true,
        "id": "jqxTRx7LrhFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(chest_Xray_train_set, epochs = 1, validation_data = chest_Xray_dev_set)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-01T12:10:14.522472Z",
          "iopub.execute_input": "2023-12-01T12:10:14.522863Z",
          "iopub.status.idle": "2023-12-01T12:33:57.162395Z",
          "shell.execute_reply.started": "2023-12-01T12:10:14.522831Z",
          "shell.execute_reply": "2023-12-01T12:33:57.161300Z"
        },
        "trusted": true,
        "id": "JKOGNycKrhFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_total = []\n",
        "y_true = []\n",
        "for img, label in chest_Xray_test_set:\n",
        "    y_pred = model.predict(img)\n",
        "    y_pred_total += [1 if i>0.5 else 0 for i in y_pred]\n",
        "    y_true += np.array(label).flatten().tolist()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-01T12:34:47.761928Z",
          "iopub.execute_input": "2023-12-01T12:34:47.762325Z",
          "iopub.status.idle": "2023-12-01T12:38:01.492128Z",
          "shell.execute_reply.started": "2023-12-01T12:34:47.762293Z",
          "shell.execute_reply": "2023-12-01T12:38:01.490851Z"
        },
        "trusted": true,
        "id": "r4hVywxFrhFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "accuracy_score(y_true, y_pred_total)*100"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-01T12:39:20.540896Z",
          "iopub.execute_input": "2023-12-01T12:39:20.541301Z",
          "iopub.status.idle": "2023-12-01T12:39:20.978689Z",
          "shell.execute_reply.started": "2023-12-01T12:39:20.541269Z",
          "shell.execute_reply": "2023-12-01T12:39:20.977790Z"
        },
        "trusted": true,
        "id": "kM7txlenrhFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision_score(y_true, y_pred_total)*100"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-01T12:39:23.107870Z",
          "iopub.execute_input": "2023-12-01T12:39:23.108292Z",
          "iopub.status.idle": "2023-12-01T12:39:23.118507Z",
          "shell.execute_reply.started": "2023-12-01T12:39:23.108235Z",
          "shell.execute_reply": "2023-12-01T12:39:23.117627Z"
        },
        "trusted": true,
        "id": "8p1HqND5rhFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recall_score(y_true, y_pred_total)*100"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-01T12:39:25.151704Z",
          "iopub.execute_input": "2023-12-01T12:39:25.152122Z",
          "iopub.status.idle": "2023-12-01T12:39:25.162542Z",
          "shell.execute_reply.started": "2023-12-01T12:39:25.152092Z",
          "shell.execute_reply": "2023-12-01T12:39:25.161569Z"
        },
        "trusted": true,
        "id": "fq7CPRABrhFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_true, y_pred_total)*100"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-01T12:39:27.628555Z",
          "iopub.execute_input": "2023-12-01T12:39:27.628949Z",
          "iopub.status.idle": "2023-12-01T12:39:27.639936Z",
          "shell.execute_reply.started": "2023-12-01T12:39:27.628921Z",
          "shell.execute_reply": "2023-12-01T12:39:27.638905Z"
        },
        "trusted": true,
        "id": "LSrDYk6NrhFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### VGG-19"
      ],
      "metadata": {
        "id": "tQdMuXS8rhFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications.vgg19 import VGG19\n",
        "vgg19 = VGG19(include_top = False, input_shape=(227,227,3))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-01T12:41:39.529519Z",
          "iopub.execute_input": "2023-12-01T12:41:39.530024Z",
          "iopub.status.idle": "2023-12-01T12:41:41.033851Z",
          "shell.execute_reply.started": "2023-12-01T12:41:39.529992Z",
          "shell.execute_reply": "2023-12-01T12:41:41.032743Z"
        },
        "trusted": true,
        "id": "diwt4MozrhFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in vgg19.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-01T12:42:19.898388Z",
          "iopub.execute_input": "2023-12-01T12:42:19.898760Z",
          "iopub.status.idle": "2023-12-01T12:42:19.905143Z",
          "shell.execute_reply.started": "2023-12-01T12:42:19.898733Z",
          "shell.execute_reply": "2023-12-01T12:42:19.904006Z"
        },
        "trusted": true,
        "id": "dkJQmhW7rhFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense, Flatten, Input\n",
        "from keras.models import Model\n",
        "x = Flatten()(vgg19.output)\n",
        "x = Dense(1024, activation = 'relu')(x)\n",
        "output = Dense(1, activation = 'sigmoid')(x)\n",
        "modelvgg19 = Model(inputs = vgg19.input, outputs = output)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-01T13:02:12.009215Z",
          "iopub.execute_input": "2023-12-01T13:02:12.009710Z",
          "iopub.status.idle": "2023-12-01T13:02:12.261858Z",
          "shell.execute_reply.started": "2023-12-01T13:02:12.009676Z",
          "shell.execute_reply": "2023-12-01T13:02:12.261066Z"
        },
        "trusted": true,
        "id": "iIv8qOourhFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelvgg19.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-01T13:03:17.468285Z",
          "iopub.execute_input": "2023-12-01T13:03:17.468721Z",
          "iopub.status.idle": "2023-12-01T13:03:17.537108Z",
          "shell.execute_reply.started": "2023-12-01T13:03:17.468688Z",
          "shell.execute_reply": "2023-12-01T13:03:17.536092Z"
        },
        "trusted": true,
        "id": "eIM0bClPrhFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelvgg19.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-01T13:04:59.754442Z",
          "iopub.execute_input": "2023-12-01T13:04:59.754855Z",
          "iopub.status.idle": "2023-12-01T13:04:59.770586Z",
          "shell.execute_reply.started": "2023-12-01T13:04:59.754825Z",
          "shell.execute_reply": "2023-12-01T13:04:59.769673Z"
        },
        "trusted": true,
        "id": "AB6RmnuFrhFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelvgg19.fit(chest_Xray_train_set, epochs = 1, validation_data = chest_Xray_dev_set)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-01T13:05:20.721307Z",
          "iopub.execute_input": "2023-12-01T13:05:20.721791Z",
          "iopub.status.idle": "2023-12-01T13:35:33.128354Z",
          "shell.execute_reply.started": "2023-12-01T13:05:20.721751Z",
          "shell.execute_reply": "2023-12-01T13:35:33.127301Z"
        },
        "trusted": true,
        "id": "a2H7qEFLrhFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_total = []\n",
        "y_true = []\n",
        "for img, label in chest_Xray_test_set:\n",
        "    y_pred = modelvgg19.predict(img)\n",
        "    y_pred_total += [1 if i>0.5 else 0 for i in y_pred]\n",
        "    y_true += np.array(label).flatten().tolist()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-01T13:35:47.642614Z",
          "iopub.execute_input": "2023-12-01T13:35:47.643002Z",
          "iopub.status.idle": "2023-12-01T13:41:15.964976Z",
          "shell.execute_reply.started": "2023-12-01T13:35:47.642973Z",
          "shell.execute_reply": "2023-12-01T13:41:15.964062Z"
        },
        "trusted": true,
        "id": "qe8eyzd1rhFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "print(f\"accurancy score: {accuracy_score(y_true, y_pred_total)*100}\")\n",
        "print(f\"precision score: {precision_score(y_true, y_pred_total)*100}\")\n",
        "print(f\"recall score: {recall_score(y_true, y_pred_total)*100}\")\n",
        "print(f\"f1 score: {f1_score(y_true, y_pred_total)*100}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-01T13:42:36.071814Z",
          "iopub.execute_input": "2023-12-01T13:42:36.072229Z",
          "iopub.status.idle": "2023-12-01T13:42:36.093087Z",
          "shell.execute_reply.started": "2023-12-01T13:42:36.072198Z",
          "shell.execute_reply": "2023-12-01T13:42:36.091873Z"
        },
        "trusted": true,
        "id": "XnJs9WQfrhFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet 50"
      ],
      "metadata": {
        "id": "nAxps4VZrhFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50V2\n",
        "resnet50 = ResNet50V2(weights = \"imagenet\", input_shape = (227,227,3), include_top = False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-02T08:21:37.034833Z",
          "iopub.execute_input": "2023-12-02T08:21:37.035400Z",
          "iopub.status.idle": "2023-12-02T08:21:39.666219Z",
          "shell.execute_reply.started": "2023-12-02T08:21:37.035355Z",
          "shell.execute_reply": "2023-12-02T08:21:39.664579Z"
        },
        "trusted": true,
        "id": "pmpR2ujmrhFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in resnet50.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-02T08:21:44.260075Z",
          "iopub.execute_input": "2023-12-02T08:21:44.260583Z",
          "iopub.status.idle": "2023-12-02T08:21:44.276228Z",
          "shell.execute_reply.started": "2023-12-02T08:21:44.260546Z",
          "shell.execute_reply": "2023-12-02T08:21:44.274816Z"
        },
        "trusted": true,
        "id": "QXKi8zw-rhFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense, Flatten, Input\n",
        "from keras.models import Model\n",
        "x = Flatten()(resnet50.output)\n",
        "x  = Dense(1024, activation = 'relu')(x)\n",
        "output = Dense(1, activation = 'sigmoid')(x)\n",
        "model_resnet50 = Model(inputs = resnet50.input, outputs = output)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-02T08:21:45.302876Z",
          "iopub.execute_input": "2023-12-02T08:21:45.303341Z",
          "iopub.status.idle": "2023-12-02T08:21:47.469442Z",
          "shell.execute_reply.started": "2023-12-02T08:21:45.303302Z",
          "shell.execute_reply": "2023-12-02T08:21:47.468005Z"
        },
        "trusted": true,
        "id": "QodZ8oiArhFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_resnet50.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-02T08:21:47.472161Z",
          "iopub.execute_input": "2023-12-02T08:21:47.473590Z",
          "iopub.status.idle": "2023-12-02T08:21:47.496870Z",
          "shell.execute_reply.started": "2023-12-02T08:21:47.473511Z",
          "shell.execute_reply": "2023-12-02T08:21:47.495528Z"
        },
        "trusted": true,
        "id": "uc4e44F7rhFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_resnet50.fit(chest_Xray_train_set, epochs=1, validation_data=chest_Xray_dev_set)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-02T08:21:48.457868Z",
          "iopub.execute_input": "2023-12-02T08:21:48.458345Z",
          "iopub.status.idle": "2023-12-02T08:32:33.866091Z",
          "shell.execute_reply.started": "2023-12-02T08:21:48.458310Z",
          "shell.execute_reply": "2023-12-02T08:32:33.864717Z"
        },
        "trusted": true,
        "id": "DwjnESANrhFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_total = []\n",
        "y_true = []\n",
        "for img, label in chest_Xray_test_set:\n",
        "    y_pred = model_resnet50.predict(img)\n",
        "    y_pred_total += [1 if i>0.5 else 0 for i in y_pred]\n",
        "    y_true += np.array(label).flatten().tolist()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-02T08:32:33.868258Z",
          "iopub.execute_input": "2023-12-02T08:32:33.869022Z",
          "iopub.status.idle": "2023-12-02T08:33:55.802430Z",
          "shell.execute_reply.started": "2023-12-02T08:32:33.868981Z",
          "shell.execute_reply": "2023-12-02T08:33:55.801011Z"
        },
        "trusted": true,
        "id": "ooRrfQ0XrhFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "print(f\"accurancy score: {accuracy_score(y_true, y_pred_total)*100}\")\n",
        "print(f\"precision score: {precision_score(y_true, y_pred_total)*100}\")\n",
        "print(f\"recall score: {recall_score(y_true, y_pred_total)*100}\")\n",
        "print(f\"f1 score: {f1_score(y_true, y_pred_total)*100}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-02T08:39:52.064199Z",
          "iopub.execute_input": "2023-12-02T08:39:52.066206Z",
          "iopub.status.idle": "2023-12-02T08:39:52.974557Z",
          "shell.execute_reply.started": "2023-12-02T08:39:52.066123Z",
          "shell.execute_reply": "2023-12-02T08:39:52.971499Z"
        },
        "trusted": true,
        "id": "oAoNWfzTrhFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## So sánh và kết luận"
      ],
      "metadata": {
        "id": "IDJF8N-jrhFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Câu 2"
      ],
      "metadata": {
        "id": "s8jro39rrhFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Pillow\n",
        "from PIL import Image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6FbuRZhC97M",
        "outputId": "c8326f71-4441-4547-f62e-f579eec52d44"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.datasets import ImageFolder"
      ],
      "metadata": {
        "id": "MVVjkoUsDHDA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/princesegzy01/Jewellery-Classification.git"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-04T06:30:23.201640Z",
          "iopub.execute_input": "2023-12-04T06:30:23.202224Z",
          "iopub.status.idle": "2023-12-04T06:30:28.099798Z",
          "shell.execute_reply.started": "2023-12-04T06:30:23.202197Z",
          "shell.execute_reply": "2023-12-04T06:30:28.098552Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ0fSE9jrhFw",
        "outputId": "f2356b5b-939e-4b37-a48d-f92fbe32d099"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Jewellery-Classification'...\n",
            "remote: Enumerating objects: 1831, done.\u001b[K\n",
            "remote: Total 1831 (delta 0), reused 0 (delta 0), pack-reused 1831\u001b[K\n",
            "Receiving objects: 100% (1831/1831), 49.51 MiB | 6.80 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.Resize((224, 224)),transforms.ToTensor()])"
      ],
      "metadata": {
        "id": "yxztyJo0CxDl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = ImageFolder(root='/content/Jewellery-Classification/dataset/training', transform=transform)\n",
        "data_test = ImageFolder(root='/content/Jewellery-Classification/dataset/test', transform=transform)"
      ],
      "metadata": {
        "id": "ddTQ8SORD4z0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(data_train, batch_size=8, shuffle=True, num_workers = 4)\n",
        "test_dataloader = DataLoader(data_test, batch_size=8, shuffle=False, num_workers = 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJMWsKPoFkpD",
        "outputId": "1b7531db-c98a-4a45-e65c-7f98e08f72b3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VGG16"
      ],
      "metadata": {
        "id": "bNmRxZGPEwdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vgg16_weights = models.VGG16_Weights.DEFAULT\n",
        "modelvgg16 = models.vgg16(weights = vgg16_weights)"
      ],
      "metadata": {
        "id": "nm3GuANcD_md"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in modelvgg16.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "-eo8_HXREyTA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelvgg16.classifier[6] = nn.Linear(4096, 5)"
      ],
      "metadata": {
        "id": "ClFoYQm4E431"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "aUpqLWJLKRlm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelvgg16.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAbdnIC_KtgP",
        "outputId": "e9c7bf26-80a6-40a4-929c-1f0c44389045"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=5, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(modelvgg16.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "qh5apCvdE81E"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_dataloader))"
      ],
      "metadata": {
        "id": "bQcSpbFzPrxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "    modelvgg16.train()\n",
        "    running_loss = 0.0\n",
        "    total = 0.0\n",
        "    correct_predictions = 0\n",
        "    for inputs, labels in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = modelvgg16(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        correct_predictions += (outputs.argmax(dim = 1) == labels).sum().item()\n",
        "        total += len(labels)\n",
        "    print(f\"Epoch {epoch+1}/{10}, Loss: {running_loss/len(train_dataloader)}, accuracy{correct_predictions/total}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrfXwrEnGNuR",
        "outputId": "52756a8d-8633-4fe3-9b0a-017a386d8fbe"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  \n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2, Loss: 0.09664328496951291, accuracy0.9629629629629629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/2, Loss: 0.06946933483399037, accuracy0.9763729246487867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/2, Loss: 0.07115857886852535, accuracy0.9738186462324393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/2, Loss: 0.06975944589096483, accuracy0.9706257982120051\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/2, Loss: 0.08095566845197621, accuracy0.9725415070242657\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/2, Loss: 0.08091226437771147, accuracy0.9699872286079183\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/2, Loss: 0.07239237312096149, accuracy0.9731800766283525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/2, Loss: 0.07580162361777258, accuracy0.9757343550446999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/2, Loss: 0.0670475174385724, accuracy0.9719029374201787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/2, Loss: 0.07181076836007248, accuracy0.9706257982120051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelvgg16.eval()\n",
        "correct_predictions = 0\n",
        "total = 0\n",
        "label_predicted = []\n",
        "label_true = []\n",
        "with torch.no_grad():\n",
        "  for img, label in test_dataloader:\n",
        "    img = img.to(device)\n",
        "    label = label.to(device)\n",
        "    output = modelvgg16(img)\n",
        "    label_predicted.append(output.argmax(dim = 1))\n",
        "    label_true.append(label)\n",
        "    correct_predictions += (output.argmax(dim = 1) == label).sum().item()\n",
        "    total += len(label)\n",
        "  print(\"accuracy: \", correct_predictions/total *100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7Dtd6GlMyle",
        "outputId": "c95e1927-d3f7-4cc1-9f16-83d09ff08847"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy:  94.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VGG19"
      ],
      "metadata": {
        "id": "nLKljmUDaoC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vgg19_weights = models.VGG19_Weights.DEFAULT\n",
        "modelvgg19 = models.vgg19(weights = vgg19_weights)\n",
        "\n",
        "for param in modelvgg19.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lLRkKYeacj9",
        "outputId": "323fb16b-3d98-4fa4-babb-93a23b8b0b1f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:05<00:00, 107MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelvgg19.classifier[6] = nn.Linear(4096, 5)"
      ],
      "metadata": {
        "id": "XVBi3_5Kbom9"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelvgg19.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dBkoli7a7NY",
        "outputId": "2965e0ad-cfcc-448f-d498-7cb2919ea85c"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (24): ReLU(inplace=True)\n",
              "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (26): ReLU(inplace=True)\n",
              "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (31): ReLU(inplace=True)\n",
              "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (33): ReLU(inplace=True)\n",
              "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (35): ReLU(inplace=True)\n",
              "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "    modelvgg19.train()\n",
        "    running_loss = 0.0\n",
        "    total = 0.0\n",
        "    correct_predictions = 0\n",
        "    for inputs, labels in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = modelvgg19(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        correct_predictions += (outputs.argmax(dim = 1) == labels).sum().item()\n",
        "        total += len(labels)\n",
        "    print(f\"Epoch {epoch+1}/{10}, Loss: {running_loss/len(train_dataloader)}, accuracy{correct_predictions/total * 100}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u6C16iJa_d_",
        "outputId": "7d611d10-fc8d-4d2e-9e1f-6cbc9c950c9d"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.61729910787271, accuracy27.84163473818646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10, Loss: 1.6145918150337375, accuracy26.81992337164751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10, Loss: 1.6195092547912986, accuracy27.011494252873565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10, Loss: 1.6117813362150777, accuracy27.522349936143037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10, Loss: 1.6142560048979155, accuracy25.351213282247762\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10, Loss: 1.611522956162083, accuracy26.81992337164751\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10, Loss: 1.6165547012066355, accuracy26.372924648786718\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10, Loss: 1.6104474067687988, accuracy25.67049808429119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10, Loss: 1.621175888241554, accuracy26.245210727969347\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Loss: 1.6134441470613285, accuracy25.67049808429119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelvgg19.eval()\n",
        "correct_predictions = 0\n",
        "total = 0\n",
        "label_predicted = []\n",
        "label_true = []\n",
        "with torch.no_grad():\n",
        "  for img, label in test_dataloader:\n",
        "    img = img.to(device)\n",
        "    label = label.to(device)\n",
        "    output = modelvgg19(img)\n",
        "    label_predicted.append(output.argmax(dim = 1))\n",
        "    label_true.append(label)\n",
        "    correct_predictions += (output.argmax(dim = 1) == label).sum().item()\n",
        "    total += len(label)\n",
        "  print(\"accuracy: \", correct_predictions/total *100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQzuKWcNbQwY",
        "outputId": "58b45667-43f7-4fad-a09f-d1cdd75c9d58"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy:  35.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet50"
      ],
      "metadata": {
        "id": "TCyoyWENcsI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet50_weights = models.ResNet50_Weights.DEFAULT\n",
        "modelReNet50 = models.resnet50(weights = resnet50_weights)"
      ],
      "metadata": {
        "id": "3t0NG5Gmcrb2"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in modelReNet50.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "2e8d9C8UlqZQ"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelReNet50.fc = nn.Linear(2048,5)"
      ],
      "metadata": {
        "id": "zXvhT2qalynw"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelReNet50.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6zTo10Il4L2",
        "outputId": "b8219d47-a279-4bbd-b667-749c50257eff"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "    modelReNet50.train()\n",
        "    running_loss = 0.0\n",
        "    total = 0.0\n",
        "    correct_predictions = 0\n",
        "    for inputs, labels in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = modelReNet50(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        correct_predictions += (outputs.argmax(dim = 1) == labels).sum().item()\n",
        "        total += len(labels)\n",
        "    print(f\"Epoch {epoch+1}/{10}, Loss: {running_loss/len(train_dataloader)}, accuracy{correct_predictions/total * 100}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6Fd3HIVmKk5",
        "outputId": "c35bbc4b-dbe9-4c1b-9953-054dafd85356"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  \n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.6133123411207784, accuracy14.240102171136654\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10, Loss: 1.6135922281109556, accuracy14.75095785440613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10, Loss: 1.61538657181117, accuracy14.687100893997446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10, Loss: 1.6127759242544368, accuracy14.687100893997446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10, Loss: 1.6114737531360315, accuracy15.453384418901662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10, Loss: 1.6134266129561834, accuracy14.559386973180077\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10, Loss: 1.6150033194191602, accuracy15.708812260536398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10, Loss: 1.6152620431111784, accuracy15.32567049808429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10, Loss: 1.6125297588961465, accuracy14.942528735632186\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Loss: 1.6127674974957291, accuracy15.32567049808429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelReNet50.eval()\n",
        "correct_predictions = 0\n",
        "total = 0\n",
        "label_predicted = []\n",
        "label_true = []\n",
        "with torch.no_grad():\n",
        "  for img, label in test_dataloader:\n",
        "    img = img.to(device)\n",
        "    label = label.to(device)\n",
        "    output = modelReNet50(img)\n",
        "    label_predicted.append(output.argmax(dim = 1))\n",
        "    label_true.append(label)\n",
        "    correct_predictions += (output.argmax(dim = 1) == label).sum().item()\n",
        "    total += len(label)\n",
        "  print(\"accuracy: \", correct_predictions/total *100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfiuLhJdmYIa",
        "outputId": "2b603f7c-d0e1-49e3-c006-6cd92b42ff31"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy:  18.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T3TFGvpTpbul"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}